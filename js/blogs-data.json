[
  {
    "id": "enhancing-logical-reasoning-llms-logic-lm",
    "filename": "enhancing-logical-reasoning-llms-logic-lm.md",
    "frontmatter": {
      "title": "Enhancing Logical Reasoning in Large Language Models with LOGIC-LM",
      "date": "2025-05-28",
      "keywords": "Large Language Models, LLMs, logical reasoning, symbolic AI, neuro-symbolic, LOGIC-LM, AI research",
      "summary": "Explore LOGIC-LM, a novel framework that significantly enhances the logical reasoning capabilities of Large Language Models by integrating them with symbolic solvers, leading to more faithful and robust AI."
    },
    "content": "## The Quest for True Logical Reasoning in AI\n\nLarge Language Models (LLMs) have shown remarkable progress in various natural language tasks, often exhibiting seemingly intelligent reasoning. However, when faced with complex logical problems, these models can falter, generating reasoning steps that don't logically support their conclusions – a phenomenon known as \"unfaithful\" reasoning. This limitation stems from their probabilistic nature, lacking the inherent guarantees of logical inference.\n\nThe field of symbolic AI, on the other hand, excels in faithful and transparent reasoning using well-defined rules. The challenge here lies in accurately translating the ambiguity of natural language into precise symbolic representations.\n\nThis is where LOGIC-LM steps in – a novel framework that elegantly bridges the gap between the natural language understanding of LLMs and the rigorous inference of symbolic solvers.\n\n## LOGIC-LM: Uniting Language and Logic\n\nLOGIC-LM, as visually depicted in Figure 1 of the [LOGIC-LM paper](https://arxiv.org/pdf/2305.12295), proposes a three-stage process to tackle logical reasoning: Problem Formulation, Symbolic Reasoning, and Result Interpretation. This architecture allows for a synergistic approach, leveraging the strengths of both LLMs and symbolic systems.\n\n### How LOGIC-LM Works: A Deep Dive into the Stages\n\nFigure 2 in the [LOGIC-LM paper](https://arxiv.org/pdf/2305.12295) provides a more detailed look at the workflow within LOGIC-LM. Let's break down each stage:\n\n1. **Problem Formulator:** An LLM, guided by specific instructions and in-context examples demonstrating the target symbolic language, takes a natural language problem and converts it into a structured symbolic formulation. This involves identifying key entities, facts, and rules within the problem statement. The paper mentions providing demonstrations with detailed instructions about the grammar of the symbolic language.\n\n * For example, the input to the LLM for formulation might include examples like:\n ```\n // In-context example format mentioned in the paper\n SYMBOLIC_FORMULA ::: NL_STATEMENT\n\n // Specific example for Logic Programming\n ConductElectricity(x, True) :- Metal(x, True). ::: If something is a metal, it conducts electricity.\n Metal(Nails, True). ::: Nails are metal.\n Query: ConductElectricity(Nails, ?). ::: Do nails conduct electricity?\n ```\n\n2. **Symbolic Reasoner:** Once the problem is in a formal symbolic language, a dedicated symbolic solver takes over. This solver performs logical inference based on the provided formulation, deriving a symbolic result with guaranteed faithfulness. The specific solver used depends on the type of logical problem and the chosen symbolic representation.\n\n3. **Result Interpreter:** The symbolic output from the solver, while logically sound, needs to be translated back into natural language to be understandable to humans. This stage uses predefined rules or even another LLM to provide the final answer in a human-readable format.\n\n### Learning from Errors: The Power of Self-Refinement\n\nA crucial aspect of LOGIC-LM is its self-refinement module. If the Problem Formulator generates an incorrect symbolic representation that leads to errors in the Symbolic Reasoner, these error messages are fed back to the LLM. The LLM then attempts to revise its symbolic formulation based on this feedback, iteratively refining its output until a valid and logically sound representation is achieved or a maximum number of attempts is reached. This process mirrors debugging in software development, making the framework more robust.\n\n* A simplified self-refinement interaction might look like this:\n ```\n // Initial LLM output for an FOL problem\n Generated_FOL: Exists x (Man(x) AND Mortal(x) AND Tall(x // Missing parenthesis\n Error_Message_From_Solver: \"Syntax error: Unbalanced parenthesis near 'Tall(x'\"\n\n // LLM prompt for refinement\n Refine_Prompt: \"The following FOL formula resulted in an error:\n Formula: Exists x (Man(x) AND Mortal(x) AND Tall(x\n Error: Syntax error: Unbalanced parenthesis near 'Tall(x'\n Please correct the formula. Common errors include missing quantifiers or unbalanced parentheses.\n Corrected_FOL:\"\n\n // LLM's refined output\n Refined_FOL: Exists x (Man(x) AND Mortal(x) AND Tall(x))\n ```\n\n### Diverse Tools for Diverse Problems: Symbolic Languages and Solvers\n\nLOGIC-LM isn't limited to a single type of logic. It strategically employs four different symbolic formulations and their corresponding solvers to handle a wide range of logical reasoning challenges:\n\n * **Logic Programming (LP) with Pyke:** For deductive reasoning, problems are encoded using a Prolog-like language (facts, rules, queries), and the Pyke expert system performs inference using forward and backward chaining. This is effective for tasks requiring logical deduction from a set of rules.\n\n * A Pyke interaction (conceptual):\n ```python\n # Pseudocode for Pyke interaction\n from pyke import knowledge_engine\n\n engine = knowledge_engine.engine(__file__)\n engine.add_source_file('rules.krb') # Contains LP rules like \"parent(X,Y) :- father(X,Y).\"\n engine.add_source_file('facts.kfb') # Contains LP facts like \"father(john, mary).\"\n\n engine.activate('bc_rules') # Activate rule base for backward chaining\n with engine.prove_goal('parent(john, $child)') as gen: # Query\n for vars, plan in gen:\n print(f\"John is a parent of: {vars['child']}\")\n ```\n\n * **First-Order Logic (FOL) with Prover9:** For more expressive logical problems, natural language is translated into FOL premises and a conclusion. Prover9, an automated theorem prover, then attempts to prove the conclusion from the premises using resolution-based inference.\n\n * Prover9 input (conceptual):\n ```prover9\n % Prover9 input file example\n formulas(assumptions).\n all x (Dog(x) -> Mammal(x)). % Premise 1\n Dog(fido). % Premise 2\n end_of_list.\n\n formulas(goals).\n Mammal(fido). % Conclusion to prove\n end_of_list.\n ```\n\n * **Constraint Satisfaction (CSP) with `python-constraint`:** Problems involving finding assignments to variables that satisfy a set of constraints are formulated as CSPs (defined by variables, domains, and constraints). The `python-constraint` library provides various solvers to find valid assignments.\n\n * `python-constraint` usage (conceptual):\n ```python\n # Pseudocode for python-constraint\n from constraint import Problem\n\n problem = Problem()\n # Variables: tractor, minivan, convertible from Figure 2\n problem.addVariables([\"tractor\", \"minivan\", \"convertible\"], [1, 2, 3]) # Domain: 1 (oldest) to 3 (newest)\n\n # Constraints from Figure 2\n problem.addConstraint(lambda t: t == 2, (\"tractor\",))\n problem.addConstraint(lambda m, c: m > c, (\"minivan\", \"convertible\"))\n problem.addConstraint(AllDifferentConstraint())\n\n solutions = problem.getSolutions()\n for sol in solutions:\n print(sol) # e.g., {'convertible': 1, 'tractor': 2, 'minivan': 3}\n ```\n\n * **Boolean Satisfiability (SAT) with Z3:** For analytical reasoning and problems that can be reduced to finding a satisfying assignment for a Boolean formula, SAT solvers like Z3 are used. Z3, a powerful SMT solver, can handle more complex logical expressions beyond basic Boolean logic.\n\n * Z3-py usage (conceptual):\n ```python\n # Pseudocode for Z3-py\n from z3 import Bool, And, Or, Not, Solver\n\n # Example: (A or B) and (Not A or C)\n A = Bool('A')\n B = Bool('B')\n C = Bool('C')\n\n s = Solver()\n s.add(Or(A, B))\n s.add(Or(Not(A), C))\n\n if s.check() == sat:\n m = s.model()\n print(m)\n else:\n print(\"unsatisfiable\")\n ```\n\nThe paper highlights that the in-context learning provided to the LLMs often uses the format `SYMBOLIC_FORMULA ::: NL_STATEMENT` to aid in aligning natural language with its symbolic counterpart.\n\n * **Constraint Satisfaction (CSP) with `python-constraint`:** Problems involving finding assignments to variables that satisfy a set of constraints are formulated as CSPs (defined by variables, domains, and constraints). The `python-constraint` library provides various solvers to find valid assignments.\n\n * `python-constraint` usage (conceptual):\n ```python\n # Pseudocode for python-constraint\n from constraint import Problem\n\n problem = Problem()\n # Variables: tractor, minivan, convertible from Figure 2\n problem.addVariables([\"tractor\", \"minivan\", \"convertible\"], [1, 2, 3]) # Domain: 1 (oldest) to 3 (newest)\n\n # Constraints from Figure 2\n problem.addConstraint(lambda t: t == 2, (\"tractor\",))\n problem.addConstraint(lambda m, c: m > c, (\"minivan\", \"convertible\"))\n problem.addConstraint(AllDifferentConstraint())\n\n solutions = problem.getSolutions()\n for sol in solutions:\n print(sol) # e.g., {'convertible': 1, 'tractor': 2, 'minivan': 3}\n ```\n\n * **Boolean Satisfiability (SAT) with Z3:** For analytical reasoning and problems that can be reduced to finding a satisfying assignment for a Boolean formula, SAT solvers like Z3 are used. Z3, a powerful SMT solver, can handle more complex logical expressions beyond basic Boolean logic.\n\n * Z3-py usage (conceptual):\n ```python\n # Pseudocode for Z3-py\n from z3 import Bool, And, Or, Not, Solver\n\n # Example: (A or B) and (Not A or C)\n A = Bool('A')\n B = Bool('B')\n C = Bool('C')\n\n s = Solver()\n s.add(Or(A, B))\n s.add(Or(Not(A), C))\n\n if s.check() == sat:\n m = s.model()\n print(m)\n else:\n print(\"unsatisfiable\")\n ```\n\nThe paper highlights that the in-context learning provided to the LLMs often uses the format `SYMBOLIC_FORMULA ::: NL_STATEMENT` to aid in aligning natural language with its symbolic counterpart.\n\n### Impressive Gains: LOGIC-LM Outperforms Standalone LLMs\n\nThe effectiveness of LOGIC-LM was rigorously evaluated on several challenging logical reasoning datasets, including ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. The results were compelling: LOGIC-LM (without self-refinement) achieved an average performance increase of 39.2% over standard LLM prompting and 18.4% over CoT prompting when using GPT-3.5. With GPT-4, LOGIC-LM further improved performance by an average of 24.98% over standard prompting and 10.44% over CoT prompting.\n\nNotably, LOGIC-LM's advantage became more pronounced on problems requiring deeper reasoning, as shown in Figure 3 of the [LOGIC-LM paper](https://arxiv.org/pdf/2305.12295), indicating its robustness in handling complexity. For instance, LOGIC-LM outperformed CoT by increasingly larger margins on the ProofWriter dataset as the reasoning depth increased from 0 to 5 hops.\n\nWhile GPT-4 showed impressive reasoning capabilities on its own, LOGIC-LM still provided substantial improvements, highlighting that even the most advanced LLMs can benefit from the structured and faithful inference of symbolic solvers. Interestingly, the study found that while CoT can be helpful for some tasks, its effectiveness was limited on problems requiring more \"non-linear\" reasoning strategies (like those in FOLIO, LogicalDeduction, and AR-LSAT), where LOGIC-LM's systematic approach excelled.\n\nThe Problem Formulator, especially when powered by GPT-4, demonstrated a high degree of proficiency in translating natural language to symbolic forms for synthetic datasets (e.g., near 100% execution rate on ProntoQA and ProofWriter). However, converting real-world, expertly crafted problems (like those in FOLIO and AR-LSAT) presented a greater challenge, with lower execution rates.\n\nThe self-refinement module proved effective in catching and correcting errors in the generated symbolic formulations, increasing the average execution rate (Exe_Rate) by 5.01%.\n\n### Understanding the Nuances: Case Study and Error Analysis\n\nThe paper included a detailed case study (Figure 5 in the [LOGIC-LM paper](https://arxiv.org/pdf/2305.12295)) showcasing LOGIC-LM's ability to handle complex problem interpretations into symbolic forms. However, error analysis (examples in Figure 6 of the [LOGIC-LM paper](https://arxiv.org/pdf/2305.12295)) revealed persistent challenges in the natural language to symbolic language translation. Common errors included incorrect predicate definition in FOL, difficulty in maintaining a global understanding while forming logical symbols, misinterpretations of specific expressions, and occasional struggles with fully grasping FOL grammar rules. These findings underscore that bridging the gap between natural language and formal logic remains a non-trivial task.\n\n### The Future of Reasoning: Potential Extensions\n\nLOGIC-LM opens up exciting avenues for future research:\n\n * **Integrating More Powerful Logic Systems:** Exploring the use of more expressive frameworks like statistical relational learning (SRL) to handle uncertainty and probabilistic reasoning, such as Markov logic networks or probabilistic soft logic.\n * **Addressing Commonsense Reasoning:** Extending the framework to tackle problems requiring vast amounts of implicit commonsense knowledge, which often involves ambiguous and complex rules.\n * **Enhancing the Natural Language to Symbolic Mapping:** Developing specialized modules or fine-tuning techniques to improve the accuracy and robustness of the translation process, especially for intricate grammar structures.\n\n### Conclusion: A Significant Step Towards Reliable AI Reasoning\n\nLOGIC-LM represents a significant step forward in our quest to build AI systems capable of robust and faithful logical reasoning. By cleverly combining the strengths of Large Language Models for understanding and Symbolic Solvers for inference, this framework achieves impressive performance gains and offers a promising direction for future research in neuro-symbolic AI. As we continue to push the boundaries of artificial intelligence, approaches like LOGIC-LM will be crucial in developing systems that not only understand and generate human language but also reason logically and reliably.\n\n## Appendix: Examples of Symbolic Formulations\n\nHere are some illustrative examples of how natural language statements might be translated into different symbolic formulations within the LOGIC-LM framework.\n\n<details>\n<summary>Logic Programming (LP) Example (JSON)</summary>\n<pre><code class=\"language-json\">\n{\n\"problem_type\": \"Deductive Reasoning\",\n\"dataset_example\": \"ProntoQA, ProofWriter\",\n\"natural_language_rule\": \"If the circuit is complete and the circuit has the light bulb, then the light bulb is glowing.\",\n\"symbolic_rule_lp\": \"Glowing(LightBulb, True) :- Complete(Circuit, True), Has(Circuit, LightBulb).\",\n\"natural_language_fact\": \"Nails are made of iron.\",\n\"symbolic_fact_lp\": \"MadeOfIron(Nails, True).\",\n\"natural_language_query\": \"Is it true that nails cannot conduct electricity? (Assuming context: Metals conduct electricity. If something is made of iron, then it is metal.)\",\n\"symbolic_query_lp\": \"ConductElectricity(Nail, False).\"\n}\n</code></pre>\n</details>\n\n<details>\n<summary>First-Order Logic (FOL) Example (JSON)</summary>\n<pre><code class=\"language-json\">\n{\n\"problem_type\": \"First-Order Logic Reasoning\",\n\"dataset_example\": \"FOLIO\",\n\"natural_language_premise\": \"A Czech person wrote a book in 1946.\",\n\"symbolic_premise_fol\": \"exists x1 exists x2 (Czech(x1) & Author(x2, x1) & Book(x2) & Publish(x2, 1946)).\",\n\"natural_language_conclusion\": \"GPT3 is popular. (Assuming context: If a language model has good performance, it is used by some researchers. A work used by some researchers should be popular. BERT is a giant language model. If BERT is a giant language model, then the same for GPT3. BERT is a giant language model.)\",\n\"symbolic_conclusion_fol\": \"Popular(gpt3).\"\n}\n</code></pre>\n</details>\n\n<details>\n<summary>Constraint Satisfaction Problem (CSP) Example (JSON)</summary>\n<pre><code class=\"language-json\">\n{\n\"problem_type\": \"Constraint Satisfaction\",\n\"dataset_example\": \"LogicalDeduction\",\n\"natural_language_problem\": \"In an antique car show, there are three vehicles: a tractor, a convertible, and a minivan. The tractor is the second newest. The minivan is newer than the convertible. Which is oldest?\",\n\"symbolic_form_csp\": {\n\"variables\": \"tractor, minivan, convertible\",\n\"domains\": \"[1, 2, 3] (where 1 is oldest, 3 is newest)\",\n\"constraints\": [\n\"tractor = 2\",\n\"minivan > convertible\",\n\"AllDifferentConstraint(tractor, minivan, convertible)\"\n],\n\"query_interpretation\": \"Find value for 'convertible' that is 1.\"\n}\n}\n</code></pre>\n</details>\n\n<details>\n<summary>Boolean Satisfiability (SAT) / SMT Example (JSON)</summary>\n<pre><code class=\"language-json\">\n{\n\"problem_type\": \"Analytical Reasoning (Formulated as SAT/SMT)\",\n\"dataset_example\": \"AR-LSAT\",\n\"natural_language_statement\": \"Xena and exactly three other technicians repair radios.\",\n\"symbolic_form_smt\": \"repairs(Xena, radios) AND Count([t : technicians], t != Xena AND repairs(t, radios)) == 3.\"\n}\n</code></pre>\n</details>\n\n### Learn More\n\nFor those interested in delving deeper, you can explore the code and related resources at the official GitHub repository: [https://github.com/teacherpeterpan/Logic-LLM](https://github.com/teacherpeterpan/Logic-LLM).\n\n### Learn More\n\nFor those interested in delving deeper, you can explore the code and related resources at the official GitHub repository: [https://github.com/teacherpeterpan/Logic-LLM](https://github.com/teacherpeterpan/Logic-LLM)."
  },
  {
    "id": "grammars-of-formal-uncertainty",
    "filename": "grammars-of-formal-uncertainty.md",
    "frontmatter": {
      "title": "Grammars of Formal Uncertainty — Quantifying Trust in LLM-Generated Proofs",
      "date": "2025-07-08",
      "keywords": "LLM, SMT-LIB, PCFG, uncertainty quantification, automated reasoning, Formal Reasoning",
      "summary": "A richly annotated walkthrough of *Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks* — complete with equations, SMT-LIB snippets, metric tables, and actionable lessons for anyone auto-formalising natural-language problems.",
      "math": "true"
    },
    "content": "## 1 Why this paper matters\n\nLarge language models can now generate SMT-LIB proofs, type constraints, and even Lean tactics — yet a single missing parenthesis can transform a *valid* proof into silent nonsense. *Grammars of Formal Uncertainty* demonstrates that the very **syntax** of these generated programs can be mined for uncertainty signals that predict failure with near-perfect precision.\n\nThe results reveal a striking domain dependency:\n* On the logical benchmark **ProofWriter**, routing questions through SMT improves answer accuracy by **+34.8 percentage points**.\n* On the knowledge-heavy **FOLIO** dataset, the same approach *hurts* performance by **-44.5 percentage points** — highlighting the critical need for per-instance trust scores.\n\nThe authors propose deriving these trust scores from a *probabilistic context-free grammar* (PCFG) fitted to multiple LLM-generated samples.\n\n---\n## 2 Pipeline overview\n\nThe methodology follows a systematic five-step process:\n\n1. **Sampling**: Generate *N* candidate programs $\\{P_i\\}_{i=1}^N$ from the LLM.\n2. **Parsing**: Apply the official SMT-LIB v2 grammar $G_{\\text{SMT}}$ to convert each $P_i$ into a parse tree.\n3. **PCFG fitting**: Estimate rule probabilities and construct the mean-branch matrix $B$.\n4. **Metric extraction**: Compute 25 uncertainty signals including entropy, Rényi divergences, spectral radius $\\rho(B)$, kurtosis, and self-consistency measures.\n5. **Selective verification**: Train a lightweight logistic regressor to abstain on high-risk cases, reducing total errors by up to **100%** with less than 10% answer loss.\n\n---\n## 3 Worked example\n\nConsider the English statement: \"Everyone who studies **math** *or* **physics** and works hard will succeed.\"\n\nThe corresponding SMT-LIB formalization is:\n\n```smt2\n(set-logic UF)\n(declare-sort Person)\n(declare-fun StudiesMath (Person) Bool)\n(declare-fun StudiesPhysics (Person) Bool)\n(declare-fun WorksHard (Person) Bool)\n(declare-fun Succeeds (Person) Bool)\n(assert (forall ((x Person))\n (=> (and (or (StudiesMath x)\n (StudiesPhysics x))\n (WorksHard x))\n (Succeeds x))))\n(check-sat)\n```\n\n> **PCFG analysis** for 100 samples:\n>\n> * Grammar entropy $H(G) = 1.42$ bits \n> * Spectral radius $\\rho(B) = 0.83$ \n> \n> Both values fall within the authors' \"safe zone\" ($H < 2$, $\\rho < 1$), indicating the program should be passed to the solver with high confidence.\n\n---\n## 4 Key uncertainty metrics\n\n| Symbol | Formula | Interpretation |\n|--------|---------|----------------|\n| $H(G)$ | Shannon entropy over non-terminal rule probabilities | Global syntactic unpredictability |\n| $\\rho(B)$ | Spectral radius of PCFG mean matrix | Depth and recursion complexity |\n| NSUI | $\\displaystyle \\frac{H(G)}{H_{\\max}} \\times \\frac{\\rho(B)}{1+\\rho(B)}$ | Combined entropy-recursion measure |\n| Self-Consistency-SMT | Agreement rate between sampled solver outputs | Behavioral reliability |\n\nComplete derivations, including Rényi-$\\alpha$ entropies, KL divergences, skewness, and kurtosis measures, are provided in Section 2.1 of the paper.\n\n---\n## 5 Experimental results\n\n### 5.1 Accuracy comparison: natural language vs. SMT formalization\n\n| Model | StrategyQA | ProntoQA | ProofWriter | FOLIO |\n|-------|------------|----------|-------------|-------|\n| **o3-mini** | 0.783 → **0.798** | 1.000 → **0.998** | 0.889 → **0.942** | **0.945** → 0.500 |\n| DeepSeek v3 | **0.829** → 0.672 | **1.000** → 0.450 | **0.806** → 0.580 | **0.933** → 0.596 |\n\n*Bold values indicate better performance. The results show significant domain-dependent effects of SMT formalization.*\n\n### 5.2 Uncertainty detection performance on ProofWriter (o3-mini)\n\n* Grammar entropy achieves AUROC = 0.93 for error detection. \n Abstaining on the riskiest 10% of cases yields **100% error-free** predictions on the remaining instances.\n* A three-metric ensemble (combining multiple uncertainty signals) improves AUROC to 0.995.\n\n---\n## 6 Key insights and implications\n\n* **Syntactic atypicality indicates semantic risk**: Low-probability grammar rules strongly correlate with incorrect proofs, providing a reliable signal for uncertainty quantification.\n\n* **Task-dependent uncertainty signals**: Different metrics excel on different domains — entropy dominates on ProofWriter, kurtosis proves strongest on StrategyQA, while spectral radius $\\rho(B)$ effectively identifies problematic arithmetic programs on ProntoQA.\n\n* **Calibration challenges**: High AUROC performance does not guarantee well-calibrated probability estimates (e.g., entropy ECE ≈ 0.44). Post-processing techniques are essential for reliable uncertainty quantification.\n\n* **Dual reasoning pathways**: The divergent error patterns between text-based and SMT-based reasoning suggest LLMs may employ two loosely coupled reasoning mechanisms. Aligning these pathways represents a promising research direction.\n\n---\n## 7 Practical implementation guide\n\nFor practitioners looking to implement this approach:\n\n1. **Sample generation**: Generate 20–30 SMT-LIB variants per input using temperature $T \\approx 0.7$.\n2. **Grammar fitting**: Construct a Laplace-smoothed PCFG from the collected samples.\n3. **Feature extraction**: Compute key metrics including $H(G)$, $\\rho(B)$, and self-consistency scores.\n4. **Threshold learning**: Train a lightweight logistic regressor on approximately 1,000 labeled examples; abstain when $P(\\text{error}) > 0.5$.\n\nThis lightweight approach enables reliable uncertainty quantification without significant computational overhead.\n\n---\n## 8 Citation\n\n```bibtex\n@misc{ganguly2025grammarsformaluncertaintytrust,\n title = {Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks},\n author = {Debargha Ganguly and Vikash Singh and Sreehari Sankar and Biyao Zhang and Xuecen Zhang and Srinivasan Iyengar and Xiaotian Han and Amit Sharma and Shivkumar Kalyanaraman and Vipin Chaudhary},\n year = {2025},\n eprint = {2505.20047},\n archivePrefix= {arXiv},\n primaryClass = {cs.CL},\n url = {https://arxiv.org/abs/2505.20047}\n}\n```\n\n*Happy formalizing!*"
  },
  {
    "id": "my-first-post",
    "filename": "my-first-post.md",
    "frontmatter": {
      "title": "Welcome to My Blog!",
      "date": "2025-05-28",
      "keywords": "introduction, portfolio, research",
      "summary": "This is the first post on my new blog. I'll be sharing updates about my research, projects, and other interests."
    },
    "content": "## Hello World!\n\nThis is the inaugural post on my personal blog. I'm excited to have this space to share updates on my journey as a PhD student and researcher in the field of Artificial Intelligence and Machine Learning.\n\n### What to Expect\n\nHere, I plan to:\n\n* Discuss my ongoing research projects.\n* Share insights from papers I'm reading.\n* Post tutorials or explanations of complex concepts.\n* Reflect on trends in AI and technology.\n\nStay tuned for more content!"
  }
]